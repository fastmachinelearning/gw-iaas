[tool.poetry]
name = "deepcleaner"
version = "0.1.0"
description = "Pipeline for running DeepClean online in production"
authors = ["Alec Gunny <alec.gunny@gmail.com>"]

[tool.poetry.scripts]
deepclean = "deepcleaner.pipeline:main"

[tool.poetry.dependencies]
python = ">=3.8,<3.10"
scipy = "^1.7"

# build tables for local dependencies
# explicitly because of how long the
# paths are
[tool.poetry.dependencies."hermes.gwftools"]
path = "../../libs/hermes/hermes.gwftools"
develop = true

[tool.poetry.dependencies."hermes.typeo"]
path = "../../libs/hermes/hermes.typeo"
develop = true

[tool.poetry.group.dev.dependencies]
notebook = "^6.4"
rich = ">=10.13,<10.15"
ipywidgets = "^7.6.5"

# add the NGC PyPI so that tritonclient
pytest = "^6.2.5"
# can install correctly
[[tool.poetry.source]]
name = "ngc"
url = "https://pypi.ngc.nvidia.com"

[build-system]
requires = ["poetry>=1.2.0a2"]
build-backend = "poetry.masonry.api"

[tool.typeo.scripts.deepclean]
kernel_length = 1
stride_length = 0.001953125
sample_rate = 4096
max_latency = 0.5

strain_data_dir = "${DATA_DIR}/llhoft/H1"
witness_data_dir = "${DATA_DIR}/lldetchar/H1"
write_dir = "${HOME}/frames/deepcleaned/hp-search-aggregated-0.5s"
preprocess_pkl = "ppr.bin"
inference_rate = 1000
num_frames = 500
channels = "channels.txt"
sequence_id = 1001
url = "localhost:8001"
model_name = "deepclean-stream"
model_version = 1
timeout = 3
start_first = true
verbose = true
log_file = "deepclean.latency-0.5.log"

[tool.typeo.scripts.export.commands.deepclean]
kernel_length = 1
stride_length = 0.001953125
sample_rate = 4096
max_latency = 0.5

channels = "../../deepclean-production/channels.txt"
streams_per_gpu = 1
instances = 6
weights = "${WEIGHTS_PATH}"
platform = "onnxruntime_onnx"
repo_dir = "${HOME}/repos/deepclean-online-production"
